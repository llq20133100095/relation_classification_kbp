{
 "cells": [
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "import os"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "import transformers\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "from tqdm.notebook import tqdm\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n",
     "name": "stderr"
    }
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "def seed_all(seed=0):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "seed = 42\n",
    "seed_all(42)"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "metadata": {
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n",
    "    \"\"\"\n",
    "    https://www.kaggle.com/xhlulu/jigsaw-tpu-distilbert-with-huggingface-and-keras\n",
    "    \"\"\"\n",
    "    tokenizer.enable_truncation(max_length=maxlen)\n",
    "    tokenizer.enable_padding(max_length=maxlen)\n",
    "    all_ids = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), chunk_size)):\n",
    "        text_chunk = texts[i:i+chunk_size].tolist()\n",
    "        encs = tokenizer.encode_batch(text_chunk)\n",
    "        all_ids.extend([enc.ids for enc in encs])\n",
    "    \n",
    "    return np.array(all_ids)"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "def regular_encode(texts, tokenizer, maxlen=512):\n",
    "    enc_di = tokenizer.batch_encode_plus(\n",
    "        texts, \n",
    "        return_attention_masks=True, \n",
    "        return_token_type_ids=True,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=maxlen\n",
    "    )\n",
    "    \n",
    "    return np.array(enc_di['input_ids'], dtype=np.int32), np.array(enc_di['attention_mask'], dtype=np.int32), np.array(enc_di[\"token_type_ids\"], dtype=np.int32)"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "def build_model(transformer, max_len=512):\n",
    "    \"\"\"\n",
    "    https://www.kaggle.com/xhlulu/jigsaw-tpu-distilbert-with-huggingface-and-keras\n",
    "    \"\"\"\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "    sequence_output = transformer((input_word_ids, input_mask, segment_ids))[0]\n",
    "    cls_token = sequence_output[:, 0, :]\n",
    "    gp = tf.keras.layers.GlobalMaxPooling1D()(sequence_output)\n",
    "    ap = tf.keras.layers.GlobalAveragePooling1D()(sequence_output)\n",
    "    stack = tf.keras.layers.concatenate([gp, ap], axis=1)\n",
    "    stack = tf.keras.layers.Dropout(0.2)(stack)\n",
    "    out = Dense(1, activation='sigmoid')(stack)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "    # model.compile(Adam(lr=0.2e-5), loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])\n",
    "    \n",
    "    return model"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## TPU Configs"
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "    # set: this is always the case on Kaggle.\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "text": "Running on TPU  grpc://10.0.0.2:8470\nREPLICAS:  8\n",
     "name": "stdout"
    }
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# Data access\n",
    "#GCS_DS_PATH = KaggleDatasets().get_gcs_path()\n",
    "\n",
    "# Configuration\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
    "MAX_LEN = 192\n",
    "# MODEL = 'jplu/tf-xlm-roberta-large'\n",
    "MODEL = '../input/jplu-tf-xlm-roberta-large'\n",
    "\n",
    "# eval\n",
    "best_auc = 0"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Create fast tokenizer"
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# First load the real tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=513.0, style=ProgressStyle(description_…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1ae7bdde86a94dcca281aa42647b95c7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "text": "\n",
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=5069051.0, style=ProgressStyle(descript…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6dc637a15ba844508afeada5783305a5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "text": "\n",
     "name": "stdout"
    }
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "text": "/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-es-cleaned.csv\n/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-it-cleaned.csv\n/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-ru.csv\n/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-fr.csv\n/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-tr.csv\n/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-it.csv\n/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-ru-cleaned.csv\n/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-tr-cleaned.csv\n/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-pt-cleaned.csv\n/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-pt.csv\n/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-es.csv\n/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-fr-cleaned.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation-processed-seqlen128.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train-processed-seqlen128.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test-processed-seqlen128.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train-processed-seqlen128.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv\n",
     "name": "stdout"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load text data into memory"
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# train1 = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\")\n",
    "# train2 = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv\")\n",
    "# train2.toxic = train2.toxic.round().astype(int)\n",
    "\n",
    "# train3 = pd.read_csv(\"/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-ru-cleaned.csv\")\n",
    "# train4 = pd.read_csv(\"/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-it-cleaned.csv\")\n",
    "# train5 = pd.read_csv(\"/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-pt-cleaned.csv\")\n",
    "# train6 = pd.read_csv(\"/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-es-cleaned.csv\")\n",
    "train7 = pd.read_csv(\"/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-tr-cleaned.csv\")\n",
    "train8 = pd.read_csv(\"/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-fr-cleaned.csv\")\n",
    "valid = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv')\n",
    "test = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv')\n",
    "sub = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv')"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "#Combine train1 with a subset of train2\n",
    "# train = pd.concat([\n",
    "#     train1[['comment_text', 'toxic']],\n",
    "#     train2[['comment_text', 'toxic']].query('toxic==1'),\n",
    "#     train2[['comment_text', 'toxic']].query('toxic==0').sample(n=100000, random_state=0)\n",
    "# ])\n",
    "\n",
    "# train_ru = pd.concat([\n",
    "#     train3[['comment_text', 'toxic']].query('toxic==1'),\n",
    "#     train3[['comment_text', 'toxic']].query('toxic==0').sample(n=100000, random_state=0)\n",
    "# ])\n",
    "# train_it = pd.concat([\n",
    "#     train4[['comment_text', 'toxic']].query('toxic==1'),\n",
    "#     train4[['comment_text', 'toxic']].query('toxic==0').sample(n=100000, random_state=0)\n",
    "# ])\n",
    "# train_pt= pd.concat([\n",
    "#     train5[['comment_text', 'toxic']].query('toxic==1'),\n",
    "#     train5[['comment_text', 'toxic']].query('toxic==0').sample(n=100000, random_state=0)\n",
    "# ])\n",
    "# train_es= pd.concat([\n",
    "#     train6[['comment_text', 'toxic']].query('toxic==1'),\n",
    "#     train6[['comment_text', 'toxic']].query('toxic==0').sample(n=100000, random_state=0)\n",
    "# ])\n",
    "train_tr= pd.concat([\n",
    "    train7[['comment_text', 'toxic']].query('toxic==1'),\n",
    "    train7[['comment_text', 'toxic']].query('toxic==0').sample(n=100000, random_state=0)\n",
    "])\n",
    "train_fr= pd.concat([\n",
    "    train8[['comment_text', 'toxic']].query('toxic==1'),\n",
    "    train8[['comment_text', 'toxic']].query('toxic==0').sample(n=100000, random_state=0)\n",
    "])"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "%%time \n",
    "\n",
    "# x_train = regular_encode(train.comment_text.values, tokenizer, maxlen=MAX_LEN)\n",
    "# x_train_ru = regular_encode(train_ru.comment_text.values, tokenizer, maxlen=MAX_LEN)\n",
    "# x_train_it = regular_encode(train_it.comment_text.values, tokenizer, maxlen=MAX_LEN)\n",
    "# x_train_pt = regular_encode(train_pt.comment_text.values, tokenizer, maxlen=MAX_LEN)\n",
    "# x_train_es = regular_encode(train_es.comment_text.values, tokenizer, maxlen=MAX_LEN)\n",
    "x_train_tr = regular_encode(train_tr.comment_text.values, tokenizer, maxlen=MAX_LEN)\n",
    "x_train_fr = regular_encode(train_fr.comment_text.values, tokenizer, maxlen=MAX_LEN)\n",
    "\n",
    "x_valid = regular_encode(valid.comment_text.values, tokenizer, maxlen=MAX_LEN)\n",
    "\n",
    "x_test = regular_encode(test.content.values, tokenizer, maxlen=MAX_LEN)\n",
    "\n",
    "\n",
    "# y_train = train.toxic.values\n",
    "# y_train_ru = train_ru.toxic.values\n",
    "# y_train_it = train_it.toxic.values\n",
    "# y_train_pt = train_pt.toxic.values\n",
    "# y_train_es = train_es.toxic.values\n",
    "y_train_tr = train_tr.toxic.values\n",
    "y_train_fr = train_fr.toxic.values\n",
    "y_valid = valid.toxic.values"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "text": "CPU times: user 12min 20s, sys: 4.06 s, total: 12min 24s\nWall time: 12min 24s\n",
     "name": "stdout"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Build datasets objects"
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# train_dataset = (\n",
    "#     tf.data.Dataset\n",
    "#     .from_tensor_slices((x_train, y_train))\n",
    "#     .repeat()\n",
    "#     .shuffle(len(train))\n",
    "#     .batch(BATCH_SIZE)\n",
    "#     .prefetch(AUTO)\n",
    "# )\n",
    "\n",
    "# train_ru_dataset = (\n",
    "#     tf.data.Dataset\n",
    "#     .from_tensor_slices((x_train_ru, y_train_ru))\n",
    "#     .repeat()\n",
    "#     .shuffle(len(train_ru))\n",
    "#     .batch(BATCH_SIZE)\n",
    "#     .prefetch(AUTO)\n",
    "# )\n",
    "# \n",
    "# train_it_dataset = (\n",
    "#     tf.data.Dataset\n",
    "#     .from_tensor_slices((x_train_it, y_train_it))\n",
    "#     .repeat()\n",
    "#     .shuffle(len(train_it))\n",
    "#     .batch(BATCH_SIZE)\n",
    "#     .prefetch(AUTO)\n",
    "# )\n",
    "# \n",
    "# train_pt_dataset = (\n",
    "#     tf.data.Dataset\n",
    "#     .from_tensor_slices((x_train_pt, y_train_pt))\n",
    "#     .repeat()\n",
    "#     .shuffle(len(train_pt))\n",
    "#     .batch(BATCH_SIZE)\n",
    "#     .prefetch(AUTO)\n",
    "# )\n",
    "# \n",
    "# train_es_dataset = (\n",
    "#     tf.data.Dataset\n",
    "#     .from_tensor_slices((x_train_es, y_train_es))\n",
    "#     .repeat()\n",
    "#     .shuffle(len(train_es))\n",
    "#     .batch(BATCH_SIZE)\n",
    "#     .prefetch(AUTO)\n",
    "# )\n",
    "# \n",
    "# train_tr_dataset = (\n",
    "#     tf.data.Dataset\n",
    "#     .from_tensor_slices((x_train_tr, y_train_tr))\n",
    "#     .repeat()\n",
    "#     .shuffle(len(train_tr))\n",
    "#     .batch(BATCH_SIZE)\n",
    "#     .prefetch(AUTO)\n",
    "# )\n",
    "# \n",
    "# train_fr_dataset = (\n",
    "#     tf.data.Dataset\n",
    "#     .from_tensor_slices((x_train_fr, y_train_fr))\n",
    "#     .repeat()\n",
    "#     .shuffle(len(train_fr))\n",
    "#     .batch(BATCH_SIZE)\n",
    "#     .prefetch(AUTO)\n",
    "# )\n",
    "\n",
    "# test_dataset = (\n",
    "#     tf.data.Dataset\n",
    "#     .from_tensor_slices(x_test)\n",
    "#     .batch(BATCH_SIZE)\n",
    "# )\n",
    "\n",
    "def get_train_dataset(batch_size, x_train, y_train):\n",
    "    train_con_dataset = (\n",
    "        tf.data.Dataset\n",
    "        .from_tensor_slices((x_train[0], x_train[1], x_train[2], y_train))\n",
    "        .repeat()\n",
    "        .shuffle(len(x_train[0]))\n",
    "        .batch(batch_size)\n",
    "        .prefetch(AUTO)\n",
    "    )\n",
    "    return train_con_dataset\n",
    "\n",
    "def get_vaild_dataset(batch_size):\n",
    "    valid_dataset = (\n",
    "        tf.data.Dataset\n",
    "        .from_tensor_slices((x_valid[0], x_valid[1], x_valid[2], y_valid))\n",
    "        .repeat()\n",
    "        .batch(batch_size)\n",
    "        .cache()\n",
    "        .prefetch(AUTO)\n",
    "    )\n",
    "    return valid_dataset\n",
    "\n",
    "per_replica_batch_size = BATCH_SIZE // strategy.num_replicas_in_sync\n",
    "\n",
    "train_tr_dataset = strategy.experimental_distribute_datasets_from_function(\n",
    "    lambda _: get_train_dataset(per_replica_batch_size, x_train_tr, y_train_tr))\n",
    "train_fr_dataset = strategy.experimental_distribute_datasets_from_function(\n",
    "    lambda _: get_train_dataset(per_replica_batch_size, x_train_fr, y_train_fr))\n",
    "\n",
    "vaild_dataset = strategy.experimental_distribute_datasets_from_function(\n",
    "    lambda _: get_vaild_dataset(per_replica_batch_size))"
   ],
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load model into the TPU"
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "%%time\n",
    "with strategy.scope():\n",
    "    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n",
    "    model = build_model(transformer_layer, max_len=MAX_LEN)\n",
    "    model.load_weights(\"../input/jigsaw-try/model.h5\")\n",
    "    optimizer = Adam(lr=2e-6)\n",
    "    training_loss = keras.metrics.Mean('training_loss', dtype=tf.float32)\n",
    "    training_accuracy = keras.metrics.BinaryAccuracy(name='training_accuracy', dtype=tf.float32)\n",
    "    training_recall = keras.metrics.Recall(name='training_recall', dtype=tf.float32)\n",
    "    training_auc= keras.metrics.AUC(name='training_auc', dtype=tf.float32)\n",
    "    valid_loss = keras.metrics.Mean('valid_loss', dtype=tf.float32)\n",
    "    valid_accuracy = keras.metrics.BinaryAccuracy(name='valid_accuracy', dtype=tf.float32)\n",
    "    valid_recall = keras.metrics.Recall(name='valid_recall', dtype=tf.float32)\n",
    "    valid_auc= keras.metrics.AUC(name='valid_auc', dtype=tf.float32)\n"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=3271420488.0, style=ProgressStyle(descr…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4551a4ea586d48838eb7ab07f5624005"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "text": "\nCPU times: user 2min 1s, sys: 36 s, total: 2min 37s\nWall time: 3min 10s\n",
     "name": "stdout"
    }
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "model.summary()"
   ],
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "text": "Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_word_ids (InputLayer)     [(None, 192)]        0                                            \n__________________________________________________________________________________________________\ninput_mask (InputLayer)         [(None, 192)]        0                                            \n__________________________________________________________________________________________________\nsegment_ids (InputLayer)        [(None, 192)]        0                                            \n__________________________________________________________________________________________________\ntf_roberta_model (TFRobertaMode ((None, 192, 1024),  559890432   input_word_ids[0][0]             \n                                                                 input_mask[0][0]                 \n                                                                 segment_ids[0][0]                \n__________________________________________________________________________________________________\nglobal_max_pooling1d (GlobalMax (None, 1024)         0           tf_roberta_model[0][0]           \n__________________________________________________________________________________________________\nglobal_average_pooling1d (Globa (None, 1024)         0           tf_roberta_model[0][0]           \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, 2048)         0           global_max_pooling1d[0][0]       \n                                                                 global_average_pooling1d[0][0]   \n__________________________________________________________________________________________________\ndropout_74 (Dropout)            (None, 2048)         0           concatenate[0][0]                \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 1)            2049        dropout_74[0][0]                 \n==================================================================================================\nTotal params: 559,892,481\nTrainable params: 559,892,481\nNon-trainable params: 0\n__________________________________________________________________________________________________\n",
     "name": "stdout"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Train Model"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "First, we train on the subset of the training set, which is completely in English.\n",
    "<p>测试集没有英文，这个可训可不训</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "TRAIN CONCAT MODEL AND SAVE MODEL"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inputs):\n",
    "    \"\"\"The step function for one training step\"\"\"\n",
    "\n",
    "    def step_fn(inputs):\n",
    "        \"\"\"The computation to run on each TPU device.\"\"\"\n",
    "        x = inputs[:3]\n",
    "        labels = tf.reshape(inputs[3], [-1, 1])\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x, training=True)\n",
    "            loss = tf.keras.losses.binary_crossentropy(labels, logits)\n",
    "            loss = tf.nn.compute_average_loss(loss, global_batch_size=BATCH_SIZE)\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(list(zip(grads, model.trainable_variables)))\n",
    "        training_loss.update_state(loss * strategy.num_replicas_in_sync)\n",
    "        training_accuracy.update_state(labels, logits)\n",
    "        training_recall.update_state(labels, logits)\n",
    "        training_auc.update_state(labels, logits)\n",
    "\n",
    "    strategy.run(step_fn, args=(next(inputs),))\n",
    "    \n",
    "@tf.function\n",
    "def valid_step(inputs):\n",
    "    \"\"\"The step function for one training step\"\"\"\n",
    "\n",
    "    def step_fn(inputs):\n",
    "        \"\"\"The computation to run on each TPU device.\"\"\"\n",
    "        x = inputs[:3]\n",
    "        labels = tf.reshape(inputs[3], [-1, 1])\n",
    "        logits = model(x, training=False)\n",
    "        loss = tf.keras.losses.binary_crossentropy(labels, logits)\n",
    "        loss = tf.nn.compute_average_loss(loss, global_batch_size=BATCH_SIZE)\n",
    "        valid_loss.update_state(loss * strategy.num_replicas_in_sync)\n",
    "        valid_accuracy.update_state(labels, logits)\n",
    "        valid_recall.update_state(labels, logits)\n",
    "        valid_auc.update_state(labels, logits)\n",
    "\n",
    "    strategy.run(step_fn, args=(next(inputs),))\n",
    "\n",
    "def train_eval(train_data_len, valid_data_len, train_iterator, valid_iterator, best_auc):\n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        for step in tqdm(range(train_data_len), desc=\"Epoch: \" + str(epoch)):\n",
    "            train_step(train_iterator)\n",
    "            print('Current step: {}, training loss: {}, accuracy: {}%, recall: {}%, auc: {}%'.format(\n",
    "                  step,\n",
    "                  round(float(training_loss.result()), 4),\n",
    "                  round(float(training_accuracy.result()) * 100, 2),\n",
    "                  round(float(training_recall.result()) * 100, 2),\n",
    "                  round(float(training_auc.result()) * 100, 2)), end='\\r')\n",
    "        \n",
    "        for step in range(valid_data_len):\n",
    "            valid_step(valid_iterator)\n",
    "        print('Valid loss: {}, accuracy: {}%, recall: {}%, auc: {}%'.format(\n",
    "            round(float(valid_loss.result()), 4),\n",
    "            round(float(valid_accuracy.result()) * 100, 2),\n",
    "            round(float(valid_recall.result()) * 100, 2),\n",
    "            round(float(valid_auc.result()) * 100, 2)), end='\\r')   \n",
    "        \n",
    "        if best_auc < round(float(valid_auc.result()) * 100, 2):\n",
    "            best_auc = round(float(valid_auc.result()) * 100, 2)\n",
    "            model.save_weights(\"model.h5\")\n",
    "        \n",
    "        training_loss.reset_states()\n",
    "        training_accuracy.reset_states()\n",
    "        training_recall.reset_states()\n",
    "        training_auc.reset_states()\n",
    "        valid_loss.reset_states()\n",
    "        valid_accuracy.reset_states()\n",
    "        valid_recall.reset_states()\n",
    "        valid_auc.reset_states()\n",
    "        \n",
    "    return best_auc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_tr_len = train_tr.shape[0] // BATCH_SIZE\n",
    "train_fr_len = train_fr.shape[0] // BATCH_SIZE\n",
    "valid_len = valid.shape[0] // BATCH_SIZE\n",
    "\n",
    "train_tr_iterator = iter(train_tr_dataset)\n",
    "train_fr_iterator = iter(train_fr_dataset)\n",
    "valid_iterator = iter(vaild_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_auc = train_eval(train_tr_len, valid_len, train_tr_iterator, valid_iterator, best_auc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_auc = train_eval(train_fr_len, valid_len, train_fr_iterator, valid_iterator, best_auc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now that we have pretty much saturated the learning potential of the model on english only data, we train it for one more epoch on the `validation` set, which is significantly smaller but contains a mixture of different languages."
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    for step in tqdm(range(valid_len), desc=\"Epoch: \" + str(epoch)):\n",
    "        train_step(valid_iterator)\n",
    "        print('Current step: {}, training loss: {}, accuracy: {}%, recall: {}%, auc: {}%'.format(\n",
    "              step,\n",
    "              round(float(training_loss.result()), 4),\n",
    "              round(float(training_accuracy.result()) * 100, 2),\n",
    "              round(float(training_recall.result()) * 100, 2),\n",
    "              round(float(training_auc.result()) * 100, 2)), end='\\r')\n",
    "        \n",
    "    training_loss.reset_states()\n",
    "    training_accuracy.reset_states()\n",
    "    training_recall.reset_states()\n",
    "    training_auc.reset_states()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Submission"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "sub['toxic'] = model.predict(x_test, verbose=1)\nsub.to_csv('submission.csv', index=False)",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}